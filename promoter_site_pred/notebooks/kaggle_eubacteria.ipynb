{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import multilabel_confusion_matrix, roc_auc_score, classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('../data/PromoterTrain.csv', index_col='id')\n",
    "y = pd.read_csv('../data/SigmaTrain.csv',index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQ</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>CAAACGCATCAGGATCAAAGTGAACATCACGAAACTTCTTACAATG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>CCGGTAAACTCTGTGGAAAGAGCAATGTGAAATCAGCGAGATAATG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>GGAATTTTCTCGAGCATAGCCAGAGCCGCAGAATTTGCTACGGTTA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>TCACCAATACCGCCTACGTCTACGCCCAGCAGTTTCAGCTTGGCGC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>GCACGGTATCGTGCTTGGTAACCTGGTAGGATTGATCGATTCTGAC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  SEQ\n",
       "id                                                   \n",
       "0   CAAACGCATCAGGATCAAAGTGAACATCACGAAACTTCTTACAATG...\n",
       "1   CCGGTAAACTCTGTGGAAAGAGCAATGTGAAATCAGCGAGATAATG...\n",
       "2   GGAATTTTCTCGAGCATAGCCAGAGCCGCAGAATTTGCTACGGTTA...\n",
       "3   TCACCAATACCGCCTACGTCTACGCCCAGCAGTTTCAGCTTGGCGC...\n",
       "4   GCACGGTATCGTGCTTGGTAACCTGGTAGGATTGATCGATTCTGAC..."
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3399 entries, 0 to 3398\n",
      "Data columns (total 1 columns):\n",
      "SEQ    3399 non-null object\n",
      "dtypes: object(1)\n",
      "memory usage: 53.1+ KB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RPOS</th>\n",
       "      <th>RPOD</th>\n",
       "      <th>RPOH</th>\n",
       "      <th>RPON</th>\n",
       "      <th>RPOF</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    RPOS  RPOD  RPOH  RPON  RPOF\n",
       "id                              \n",
       "0      1     0     0     0     0\n",
       "1      0     0     0     0     0\n",
       "2      1     1     1     0     0\n",
       "3      0     0     0     0     0\n",
       "4      1     0     0     0     0"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3399 entries, 0 to 3398\n",
      "Data columns (total 5 columns):\n",
      "RPOS    3399 non-null int64\n",
      "RPOD    3399 non-null int64\n",
      "RPOH    3399 non-null int64\n",
      "RPON    3399 non-null int64\n",
      "RPOF    3399 non-null int64\n",
      "dtypes: int64(5)\n",
      "memory usage: 159.3 KB\n"
     ]
    }
   ],
   "source": [
    "y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CAAACGCATCAGGATCAAAGTGAACATCACGAAACTTCTTACAATGGCGCA'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['SEQ'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates overlapping k-mers of len=length from each sequence\n",
    "# def create_kmers_word(seq, length):\n",
    "#     k_mers = [seq[i:i+length].lower() for i in range(len(seq) - length + 1)]\n",
    "#     kmers_word = ' '.join(k_mers).lower()\n",
    "#     return kmers_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_X['promoter_train'] = promoter_train.apply(lambda i : create_kmers_word(i['SEQ'], 6), axis=1)\n",
    "# db_y['promoter_test'] = promoter_test.apply(lambda i : create_kmers_word(i['SEQ'], 6), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define metrics for the models\n",
    "def get_metrics(y_test, y_predicted):\n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    precision = precision_score(y_test, y_predicted, average='weighted')\n",
    "    recall = recall_score(y_test, y_predicted, average='weighted')\n",
    "    f1 = f1_score(y_test, y_predicted, average='weighted')\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X['SEQ'], y, test_size = 0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the model with MultinomialNB\n",
    "steps = [('cv', CountVectorizer(analyzer='char',ngram_range=(3,7))),\n",
    "         ('ovr', OneVsRestClassifier(MultinomialNB(fit_prior=True, class_prior=None)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline_ovr = Pipeline(steps)\n",
    "ovr_clf = pipeline_ovr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = ovr_clf.predict(X_test)\n",
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[239, 103],\n",
       "        [106, 232]],\n",
       "\n",
       "       [[344, 104],\n",
       "        [ 94, 138]],\n",
       "\n",
       "       [[547,  45],\n",
       "        [ 65,  23]],\n",
       "\n",
       "       [[628,   7],\n",
       "        [ 41,   4]],\n",
       "\n",
       "       [[666,   0],\n",
       "        [ 14,   0]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.69      0.69       338\n",
      "           1       0.57      0.59      0.58       232\n",
      "           2       0.34      0.26      0.29        88\n",
      "           3       0.36      0.09      0.14        45\n",
      "           4       0.00      0.00      0.00        14\n",
      "\n",
      "   micro avg       0.61      0.55      0.58       717\n",
      "   macro avg       0.39      0.33      0.34       717\n",
      "weighted avg       0.58      0.55      0.56       717\n",
      " samples avg       0.30      0.29      0.28       717\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/claudiapacini/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/claudiapacini/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/claudiapacini/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.46 \n",
      "precision = 0.58 \n",
      "recall = 0.55 \n",
      "f1 = 0.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/claudiapacini/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/claudiapacini/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_pred_test)\n",
    "print(f'accuracy = {accuracy:.2f} \\nprecision = {precision:.2f} \\nrecall = {recall:.2f} \\nf1 = {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6011121060656911"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_score_test = roc_auc_score(y_test, y_pred_test)\n",
    "auc_score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build model with RandomForestClassifier\n",
    "steps_rfc = [('cv', CountVectorizer(ngram_range=(1,2))),\n",
    "         ('rfc', RandomForestClassifier())]\n",
    "params_rfc = {'rfc__max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90],\n",
    "              'rfc__max_features': ['auto', 'sqrt'],\n",
    "              'rfc__min_samples_leaf': [1, 2, 4],\n",
    "              'rfc__min_samples_split': [2, 5, 10],\n",
    "              'rfc__n_estimators': [50, 100, 200, 400, 600, 800, 1000]}\n",
    "pipeline_rfc = Pipeline(steps_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_rfc = RandomizedSearchCV(pipeline_rfc, params_rfc, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_clf = grid_search_rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rfc = rfc_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[342,   0],\n",
       "        [338,   0]],\n",
       "\n",
       "       [[448,   0],\n",
       "        [232,   0]],\n",
       "\n",
       "       [[592,   0],\n",
       "        [ 88,   0]],\n",
       "\n",
       "       [[635,   0],\n",
       "        [ 45,   0]],\n",
       "\n",
       "       [[666,   0],\n",
       "        [ 14,   0]]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(y_test, y_pred_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       338\n",
      "           1       0.00      0.00      0.00       232\n",
      "           2       0.00      0.00      0.00        88\n",
      "           3       0.00      0.00      0.00        45\n",
      "           4       0.00      0.00      0.00        14\n",
      "\n",
      "   micro avg       0.00      0.00      0.00       717\n",
      "   macro avg       0.00      0.00      0.00       717\n",
      "weighted avg       0.00      0.00      0.00       717\n",
      " samples avg       0.00      0.00      0.00       717\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/claudiapacini/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/claudiapacini/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/claudiapacini/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/claudiapacini/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_rfc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.47 \n",
      "precision = 0.00 \n",
      "recall = 0.00 \n",
      "f1 = 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/claudiapacini/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/claudiapacini/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_pred_rfc)\n",
    "print(f'accuracy = {accuracy:.2f} \\nprecision = {precision:.2f} \\nrecall = {recall:.2f} \\nf1 = {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_score_rfc = roc_auc_score(y_test, y_pred_rfc)\n",
    "auc_score_rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build model with kNN\n",
    "steps_knn = [('cv', CountVectorizer(analyzer='char', ngram_range=(3,7))),\n",
    "         ('knn', KNeighborsClassifier())]\n",
    "params_knn = {'knn__n_neighbors': np.arange(1,35),\n",
    "              'knn__leaf_size': [10,20,30,40],\n",
    "              'knn__p': [1,2]}\n",
    "pipeline_knn = Pipeline(steps_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cv = RandomizedSearchCV(pipeline_knn, params_knn, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('cv',\n",
       "                                              CountVectorizer(analyzer='char',\n",
       "                                                              binary=False,\n",
       "                                                              decode_error='strict',\n",
       "                                                              dtype=<class 'numpy.int64'>,\n",
       "                                                              encoding='utf-8',\n",
       "                                                              input='content',\n",
       "                                                              lowercase=True,\n",
       "                                                              max_df=1.0,\n",
       "                                                              max_features=None,\n",
       "                                                              min_df=1,\n",
       "                                                              ngram_range=(3,\n",
       "                                                                           7),\n",
       "                                                              preprocessor=None,\n",
       "                                                              stop_words=None,\n",
       "                                                              strip_accents=None...\n",
       "                                                                   weights='uniform'))],\n",
       "                                      verbose=False),\n",
       "                   iid='warn', n_iter=10, n_jobs=None,\n",
       "                   param_distributions={'knn__leaf_size': [10, 20, 30, 40],\n",
       "                                        'knn__n_neighbors': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]),\n",
       "                                        'knn__p': [1, 2]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_knn = knn_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[268,  74],\n",
       "        [203, 135]],\n",
       "\n",
       "       [[413,  35],\n",
       "        [188,  44]],\n",
       "\n",
       "       [[582,  10],\n",
       "        [ 81,   7]],\n",
       "\n",
       "       [[631,   4],\n",
       "        [ 44,   1]],\n",
       "\n",
       "       [[666,   0],\n",
       "        [ 14,   0]]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(y_test, y_pred_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.40      0.49       338\n",
      "           1       0.56      0.19      0.28       232\n",
      "           2       0.41      0.08      0.13        88\n",
      "           3       0.20      0.02      0.04        45\n",
      "           4       0.00      0.00      0.00        14\n",
      "\n",
      "   micro avg       0.60      0.26      0.36       717\n",
      "   macro avg       0.36      0.14      0.19       717\n",
      "weighted avg       0.55      0.26      0.34       717\n",
      " samples avg       0.19      0.15      0.16       717\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/claudiapacini/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/claudiapacini/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/claudiapacini/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.45 \n",
      "precision = 0.55 \n",
      "recall = 0.26 \n",
      "f1 = 0.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/claudiapacini/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/claudiapacini/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_pred_knn)\n",
    "print(f'accuracy = {accuracy:.2f} \\nprecision = {precision:.2f} \\nrecall = {recall:.2f} \\nf1 = {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5373140759708973"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_score_knn = roc_auc_score(y_test, y_pred_knn)\n",
    "auc_score_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
